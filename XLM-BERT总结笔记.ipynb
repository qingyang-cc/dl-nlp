{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLM-RoBERTa--跨(多)语言模型\n",
    "> XLM是融合了MLM和TLM两种算法的BERT模型，与RoBERTa结合，采取了RoBERTa动态掩码的方式。\n",
    "\n",
    "> `MLM：`MLM的做法和Bert的LM做法是一样的，对于句子中词进行mask预测这个mask的词来捕获上下文语义。bert中针对词的MLM分为3种方式：[MASK]，原始词和随机词。\n",
    ">- 1 首先选取所有词中的15%个数\n",
    ">- 2 15%的选择词数中，80%用[MASK]的tocken表示，10%用原始tocken表示，10%用随机tocken表示。\n",
    "\n",
    ">- `TLM：`TLM的做法则是使用将两个意思相近但是语言不同的句子进行拼接，来学习到不同语言之间的联系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码流程思路\n",
    "0. [加载包](#0)\n",
    "0. [TPU](#1)\n",
    "0. [设置超参数](#2)\n",
    "0. [定义数据Token函数](#3)\n",
    "0. [定义模型函数](#5)\n",
    "0. [加载数据集](#6)\n",
    "0. [生成训练数据类型](#4)\n",
    "0. [调用、训练模型](#7)\n",
    "0. [预测和提交](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=0></a>\n",
    "### 0.加载包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from transformers import TFXLMRobertaModel, XLMRobertaTokenizer\n",
    "from transformers import TFAutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=1></a>\n",
    "### 1.TPU\n",
    "> 调用TPU训练，代码都差不多，直接复制即可\n",
    "> - 注意使用TPU训练时，都需加上`with strategy.scope():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU detection.\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n",
    "\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync) #输出设备数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=2></a>\n",
    "### 2.设置必要参数\n",
    "model,epoch,maxlen,tokenizer,auto,batchsize,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2020\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "model_name = 'jplu/tf-xlm-roberta-large'\n",
    "#model_name = 'joeddav/xlm-roberta-large-xnli'\n",
    "\n",
    "epoch = 10\n",
    "\n",
    "maxlen = 100\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "auto = tf.data.experimental.AUTOTUNE #线程数\n",
    "batch_size = 16 * strategy.num_replicas_in_sync #用TPU时，大家都这么设 8、16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=3></a>\n",
    "### 3.定义数据Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlm_encode_1(x1, x2, maxlen):\n",
    "    '''\n",
    "    x1:df.x1.values\n",
    "    x2:df.x2.values\n",
    "    max_len: int最大长度\n",
    "    '''\n",
    "    s1 = [tokenizer.encode(x) for x in x1]\n",
    "    s2 = [tkoenizer.enocde(x) for x in x2]\n",
    "    input_word_ids = list(map(lambda x:x[0]+x[1], list(zip(s1,s2)))) #模型只接受list输入\n",
    "    input_mask = [np.ones_like(x) for x in input_word_ids]\n",
    "    inputs={\n",
    "        'input_word_ids': tf.keras.prepocessing.sequence.pad_sequences(input_word_ids, padding='post', maxlen=maxlen),\n",
    "        'input_mask': tf.keras.prepocessing.sequence.pad_sequences(input_mask, padding='post', maxlen=maxlen)\n",
    "        }\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlm_encode_2(x1, maxlen):\n",
    "    '''\n",
    "    x1: df.x1.values\n",
    "    max_len: int 最大长度\n",
    "    tokenizer = AutoTokenizer\n",
    "    '''\n",
    "    encoded = tokenizer.batch_encode_plus(x1, pad_to_max_length=True, max_length=maxlen)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=5></a>\n",
    "### 4.定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_model():\n",
    "    with strategy.scope():\n",
    "        #加载模型预训练层\n",
    "        bert_encoder = TFXLMRobertaModel.from_pretrained(model_name)\n",
    "        #transformer_encoder = TFAutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        #输入token\n",
    "        input_word_ids = tf.keras.Input(shape=(None, ), dtype=tf.int32, name='input_word_ids')\n",
    "        input_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_mask\")\n",
    "        \n",
    "        #使用第一步加载的encoder，将输入转换为Bert embedding\n",
    "        embedding = bert_encoder([input_word_ids, input_mask])[0]\n",
    "        \n",
    "        #下接自己定义的下游任务\n",
    "        output_layer = tf.keras.layers.Dropout(0.3)(embedding)\n",
    "        output_dense_layer = tf.keras.layers.Dense(64, activation='relu')(output_layer)\n",
    "        output_dense_layer = tf.keras.layers.BatchNormalization()(output_dense_layer)\n",
    "        output_dense_layer = tf.keras.layers.Dense(32, activation='relu')(output_dense_layer)\n",
    "        \n",
    "        output = tf.keras.layers.Dense(3, activation='softmax')(output_dense_layer)\n",
    "        \n",
    "        #定义模型输入\n",
    "        model = tf.keras.Model(inputs=[input_word_ids, input_mask], outputs=output)\n",
    "        \n",
    "        #定义损失等\n",
    "        model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulid_model(max_len):\n",
    "    #\n",
    "    transformer_encode = TFAutoModel.from_pretrained(model_name)\n",
    "    #\n",
    "    input_word = tf.keras.Input(shape=(max_len, ), dtype=tf.int32, name='input_word')\n",
    "    #\n",
    "    embedding = transformer_encode(input_word)[0]\n",
    "    #提取<s>\n",
    "    cls_token = sequence_output[:, 0, :]\n",
    "    #\n",
    "    \n",
    "    \n",
    "    #\n",
    "    out = Dense(3, activation='softmax')(cls_token)\n",
    "\n",
    "    model = Model(inputs=input_ids, outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = bulid_model(max_len)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=6></a>\n",
    "### 5.加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('')\n",
    "test_df = pd.read._csv('')\n",
    "\n",
    "train_text = train_df[['premise', 'hypothesis']].values.tolist()\n",
    "test_text = test_df[['premise', 'hypothesis']].values.tolist()\n",
    "\n",
    "train_label = train_df.label.values\n",
    "\n",
    "#切分训练、测试集\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train_text, train_label, test_size=0.2, random_state=2020)\n",
    "\n",
    "train_input = xlm_encode_2(train_x, maxlen)\n",
    "valid_input = xlm_encode_2(valid_x, maxlen)\n",
    "test_input = xlm_encode_2(test_text, maxlen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=4></a>\n",
    "### 6.生成数据\n",
    "- Step0: 准备要加载的numpy数据\n",
    "- Step1: 使用 tf.data.Dataset.from_tensor_slices() 函数进行加载\n",
    "- Step2: 使用 shuffle() 打乱数据\n",
    "- Step3: 使用 map() 函数进行预处理\n",
    "- Step4: 使用 batch() 函数设置 batch size 值\n",
    "- Step5: 根据需要 使用 repeat() 设置是否循环迭代数据集（一般test数据集不用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模型输入是多特征对一label的形式 tf.data.Dataset.from_tensor_slices()进行切片\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input,train_y)).repeat().shuffle(2020).batch(batch_size).prefetch(auto)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_input['input_ids']).batch(batch_size)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_input, valid_y)).batch(batch_size).cache().prefetch(auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=7></a>\n",
    "### 7.调用、训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = len(train_y) // batch_size\n",
    "\n",
    "history = model.fit(train_dataset, steps_per_epoch=n_step, validation_data=test_dataset, epochs=epoch, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=8></a>\n",
    "### 9.预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_input, verbose=1)\n",
    "\n",
    "submission = test_df.id.copy().to_frame()\n",
    "submission['prediction'] = test_preds.argmax(axis=1)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
